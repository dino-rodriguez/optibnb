{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.datasets import load_digits\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import listings, clean data, extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data \n",
    "listings = pd.read_csv('datasets/listings.csv', delimiter=',')\n",
    "calendar = pd.read_csv('datasets/calendar.csv', delimiter=',', usecols=range(4))\n",
    "\n",
    "# View feature list\n",
    "print listings.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unsuppress Output\n",
    "pd.options.display.max_columns = 70\n",
    "\n",
    "# Split into predictor and response\n",
    "y = listings[['price']]\n",
    "x = listings\n",
    "del x['price']\n",
    "x = x.join(y)\n",
    "\n",
    "\n",
    "# Show predictor df\n",
    "print 'Predictor Data Shape: ', x.shape\n",
    "x.head(n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For our baseline model, we can start by removing features that we intuitively sense will not impact a listing's price. This includes 11 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del x['scrape_id']\n",
    "del x['last_scraped'] # all within first three days of January, not necessary feature\n",
    "del x['picture_url']\n",
    "del x['host_picture_url']\n",
    "del x['neighbourhood'] # neighbourhood_cleansed is in a better format\n",
    "del x['state'] # all NY\n",
    "del x['market'] # all New York\n",
    "del x['country'] # all United States\n",
    "del x['weekly_price'] # function of daily price\n",
    "del x['monthly_price'] # function of daily price\n",
    "del x['calendar_last_scraped'] # all within first three days of January, not necessary feature\n",
    "\n",
    "print 'Predictor Data Shape: ', x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns percent of missing data in column\n",
    "def percent_empty(df):\n",
    "    bools = df.isnull().tolist()\n",
    "    percent_empty = float(bools.count(True)) / float(len(bools))\n",
    "    return percent_empty\n",
    "\n",
    "emptiness = []\n",
    "\n",
    "for i in range(0, x.shape[1]):\n",
    "    emptiness.append(round(percent_empty(x.iloc[:,i]),2))\n",
    "    \n",
    "empty_dict = dict(zip(x.columns.values.tolist(), emptiness))\n",
    "\n",
    "empty = pd.DataFrame.from_dict(empty_dict, orient = 'index').sort_values(by=0)\n",
    "ax = empty.plot(kind='bar',color='red', figsize = (16, 5))\n",
    "ax.set_xlabel('Predictor')\n",
    "ax.set_ylabel('Percent Empty/NaN')\n",
    "ax.set_title('Feature Emptiness')\n",
    "ax.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del x['square_feet'] #remove it because it has too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bedrooms_counts = Counter(x.bedrooms)\n",
    "tdf = pd.DataFrame.from_dict(bedrooms_counts, orient='index').sort_values(by=0)\n",
    "tdf = tdf.iloc[-10:, :] / 27392\n",
    "\n",
    "print tdf\n",
    "\n",
    "ax = tdf.plot(kind='bar', figsize = (12,4))\n",
    "ax.set_xlabel(\"# of Bedrooms\")\n",
    "ax.set_ylabel(\"% of Listings\")\n",
    "ax.set_title('Percent of Listings by Bedrooms #')\n",
    "ax.legend_.remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove entries (rows) that have faulty data like when\n",
    "- There are 0 bedrooms\n",
    "- There are 0 bathrooms\n",
    "- There are 0 beds\n",
    "- The price is $0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete bad entries\n",
    "x = x[x.bedrooms != 0]\n",
    "x = x[x.beds != 0]\n",
    "x = x[x.bathrooms != 0]\n",
    "x = x[x.price != 0]\n",
    "\n",
    "# Delete additional entries with NaN values\n",
    "x = x.dropna(axis=0)\n",
    "\n",
    "print 'Size of trimmed data: ', x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to drop the dollar sign from our price and turn the type into a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert $ to float for 'price'\n",
    "x['price'] = x['price'].replace('[\\$,)]','',  \\\n",
    "        regex=True).replace('[(]','-', regex=True).astype(float)\n",
    "\n",
    "# Convert $ to float for 'extra people'\n",
    "x['extra_people'] = x['extra_people'].replace('[\\$,)]','',  \\\n",
    "        regex=True).replace('[(]','-', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_fivehundred = x[x['price'] < 500]\n",
    "five_thousand = x[(x['price'] > 500) & (x['price'] < 1000)]\n",
    "thousand_fifteen = x[(x['price'] > 1000) & (x['price'] < 1500)]\n",
    "fifteen_three = x[(x['price'] > 1500) & (x['price'] < 3000)]\n",
    "three_four = x[(x['price'] > 3000) & (x['price'] < 4000)]\n",
    "four_five = x[(x['price'] > 4000) & (x['price'] < 5000)]\n",
    "\n",
    "# Let's get a feel for our data\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3, figsize = (16,9))\n",
    "\n",
    "BINS = 10\n",
    "\n",
    "ax1.hist(zero_fivehundred['price'], bins=BINS)\n",
    "ax1.set_xlabel('Listings Priced 0 to 500')\n",
    "ax2.hist(five_thousand['price'], bins=BINS)\n",
    "ax2.set_xlabel('Listings Priced 500 to 1000')\n",
    "ax3.hist(thousand_fifteen['price'], bins=BINS)\n",
    "ax3.set_xlabel('Listings Priced 1000 to 1500')\n",
    "ax4.hist(fifteen_three['price'], bins=BINS)\n",
    "ax4.set_xlabel('Listings Priced 1500 to 3000')\n",
    "ax5.hist(three_four['price'], bins=BINS)\n",
    "ax5.set_xlabel('Listings Priced 3000 to 4000')\n",
    "ax6.hist(four_five['price'], bins=BINS)\n",
    "ax6.set_xlabel('Listings Priced 4000 to 5000')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a map visualization, say we want generally even sized buckets, each of which will map to a color. By looking at our histograms let's have our buckets the intervals of [0,50,100,150,250,500,10000]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Outline price buckets\n",
    "intervals = [0,100,300,10000]\n",
    "leg_labels = []\n",
    "\n",
    "# Get Labels for Legend\n",
    "for i in range(0,len(intervals) - 1):\n",
    "    if i == len(intervals) - 2:\n",
    "        leg_labels.append('\\${}+'.format(intervals[i]))\n",
    "    else:\n",
    "        leg_labels.append(\"\\${}-\\${}\".format(intervals[i], intervals[i+1]))    \n",
    "\n",
    "buckets = []\n",
    "\n",
    "# Divide up into buckets\n",
    "for i in range(0, len(intervals) - 1):\n",
    "    buckets.append(x[(x['price'] > intervals[i]) & (x['price'] < intervals[i+1])])\n",
    "\n",
    "colors = ['red', 'blue', 'yellow', 'cyan', 'pink', 'purple']\n",
    "alphas = [.3,.2,.5,1,.4,.2]  \n",
    "\n",
    "# Plot listings on scatter\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(0, len(buckets)):\n",
    "    plt.scatter(buckets[i]['latitude'], buckets[i]['longitude'], alpha = alphas[i], c=colors[i], s=25)\n",
    "    \n",
    "plt.title('New York City - AirBnB Listings')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.legend(labels=leg_labels, loc = 'best')\n",
    "plt.ylim(-74.2,-73.7)\n",
    "plt.xlim(40.45,40.95)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Listings on Central Park:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "streets = x['street'].tolist()\n",
    "streets_cleansed = []\n",
    "\n",
    "# Clean Street Data\n",
    "for street in streets:\n",
    "    i = street.find(',')\n",
    "    streets_cleansed.append(street[:i])\n",
    "\n",
    "x['streets_cleansed'] = streets_cleansed\n",
    "\n",
    "# List of Streets on Central Park\n",
    "park_streets = ['West 110th Street', 'W 110th St', '5th Ave', '5th Avenue', 'Central Park', \n",
    "                'Central Park South', 'Central Park West', 'Central Park North', 'Central Park East', 'West 59th Street']\n",
    "\n",
    "x['on_park'] = x['streets_cleansed'].isin(park_streets)\n",
    "\n",
    "park = x[x['on_park'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verts = [\n",
    "    (-73.958546, 40.801266),\n",
    "    (-73.948807, 40.797251),\n",
    "    (-73.972904, 40.764011),\n",
    "    (-73.982735, 40.767682),\n",
    "    (0., 0.), # ignored\n",
    "        ]\n",
    "\n",
    "codes = [Path.MOVETO,\n",
    "         Path.LINETO,\n",
    "         Path.LINETO,\n",
    "         Path.LINETO,\n",
    "         Path.CLOSEPOLY,\n",
    "         ]\n",
    "\n",
    "path = Path(verts, codes)\n",
    "patch = patches.PathPatch(path,facecolor = 'none', lw=2)\n",
    "\n",
    "on_park = []\n",
    "for index, row in x.iterrows():\n",
    "    on_park.append(path.contains_point((row['longitude'], row['latitude']) , radius = -.002))\n",
    "\n",
    "    \n",
    "x['on_park'] = on_park\n",
    "park = x[x['on_park'] == True]\n",
    "\n",
    "print park.shape\n",
    "\n",
    "# Remove non-exact entries\n",
    "park = park[park['is_location_exact'] == 't']\n",
    "print park.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot listings on scatter\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot all listings by price bucket\n",
    "for i in range(0, len(buckets)):\n",
    "    ax.scatter(buckets[i]['longitude'], buckets[i]['latitude'], alpha = alphas[i], c=colors[i], s=2)\n",
    "\n",
    "# Plot Central Park Listings\n",
    "ax.scatter(park['longitude'], park['latitude'], s=20, alpha = 1, c='yellow')\n",
    "\n",
    "# Figure attributes\n",
    "ax.set_title('New York City - AirBnB Listings')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend(labels=leg_labels, loc = 'best')\n",
    "ax.add_patch(patch)\n",
    "ax.set_xlim(-74,-73.9)\n",
    "ax.set_ylim(40.75,40.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode_categorical\n",
    "# \n",
    "# Function to label encode categorical variables.\n",
    "#     Input: array (array of values)\n",
    "#     Output: array (array of encoded values)\n",
    "def encode_categorical(array):\n",
    "    if not array.dtype == np.dtype('float64'):\n",
    "        return preprocessing.LabelEncoder().fit_transform(array) \n",
    "    else:\n",
    "        return array\n",
    "\n",
    "new_x = x.apply(encode_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute matrix of correlation coefficients\n",
    "corr_matrix = np.corrcoef(new_x.T)\n",
    "\n",
    "pd.DataFrame(data = corr_matrix, columns = new_x.columns, index = new_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute matrix of correlation coefficients\n",
    "corr_matrix = np.corrcoef(new_x.T)\n",
    "\n",
    "# Display heat map \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pcolor(corr_matrix, cmap='RdBu')\n",
    "plt.xlabel('Predictor Index')\n",
    "plt.ylabel('Predictor Index')\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there is a high correlation between all of the availability features. Availability 365 has the lowest correlation to the others, so opt to keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del new_x['availability_30']\n",
    "del new_x['availability_60']\n",
    "del new_x['availability_90']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are doing OLS for our baseline regression, we must have only numerical predictors and so we must also one-hot encode our categorical variables. We also get rid of a number of the categorical variables that have too many unique combinations when one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del new_x['id']\n",
    "del new_x['host_id']\n",
    "del new_x['host_name']\n",
    "del new_x['street']\n",
    "del new_x['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply one hot endcoing\n",
    "#categorical = (new_x.dtypes.values != np.dtype('float64'))\n",
    "\n",
    "#encoder = preprocessing.OneHotEncoder(categorical_features = categorical, sparse = False)  # Last value in mask is y\n",
    "#new_x = encoder.fit_transform(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print new_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that one-hot encoding brought our number of features from 13 to 197. In actuality and as a quick sanity check, the only categorical variables are 'property_type', 'room_type', and 'neighbourhood_cleansed' and there are 186 neighbourhoods so this makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(new_x.drop('price', axis = 1), new_x.price, test_size=0.2, random_state = 41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg = LinReg()\n",
    "linreg.fit(X_train,Y_train)\n",
    "training_set_score = linreg.score(X_test,Y_test)\n",
    "print 'The R^2 score on our training data is: ' + str(round(training_set_score,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stores the coefficient values of the predictors\n",
    "#coefficient_values = np.array(linreg.coef_)\n",
    "\n",
    "# stores the names of the variables\n",
    "#variable_names = X_train.columns.values\n",
    "\n",
    "# creates table storing the coefficient values and variable names\n",
    "#coef_matrix = pd.DataFrame({'CoefValues':coefficient_values, 'VarName': variable_names, 'AbsCoef': abs(coefficient_values)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a table that contains the sorted coefficient values for each variable that we decided to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sorted_coef_matrix = coef_matrix.sort(columns='AbsCoef').drop('AbsCoef', axis=1)\n",
    "#sorted_coef_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our three categorical features have the same weight despite their encodings â€“ this would likely not be the case in a non-linear model and will be interesting to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Model (Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = 10.**np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
    "\n",
    "#Perform Ridge regression using expanded set of predictors, \n",
    "#choose best regularization parameter lambda using 5-fold x-validation\n",
    "ridge = RidgeCV(alphas=lambdas, fit_intercept=False, normalize=True, cv=5)\n",
    "ridge.fit(X_train, Y_train)\n",
    "ridge.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Lasso Model (Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perform Lasso regression using expanded set of predictors, \n",
    "#choose best regularization parameter lambda using 5-fold x-validation\n",
    "lasso = LassoCV(alphas=lambdas, tol=0.01, fit_intercept=False, normalize=True, cv=5)\n",
    "lasso.fit(X_train, Y_train)\n",
    "lasso.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 20)\n",
    "rf.fit(X_train, Y_train)\n",
    "rf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random Forest Regressor After Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_est = 20\n",
    "\n",
    "tuned_parameters = {\n",
    "    \"n_estimators\": [ n_est ]\n",
    "}\n",
    "\n",
    "rf_tuned = GridSearchCV(rf, cv=3, param_grid=tuned_parameters)\n",
    "preds = rf_tuned.fit(X_train, Y_train)\n",
    "best = rf_tuned.best_estimator_ \n",
    "rf_tuned.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostRegressor(base_estimator=rf_tuned, n_estimators=10)\n",
    "ada.fit(X_train, Y_train)\n",
    "ada.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Regressor After Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_est = 10\n",
    "\n",
    "tuned_parameters = {\n",
    "    \"base_estimator\": [ rf_tuned ],\n",
    "    \"n_estimators\": [ n_est ],\n",
    "    \"learning_rate\": [ 0.01 ],\n",
    "    \"loss\" : [ 'linear' ]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(ada, cv=3, param_grid=tuned_parameters)\n",
    "preds = clf.fit(X_train, Y_train)\n",
    "best = clf.best_estimator_ \n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^{2}$ Values Across Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rs = 1\n",
    "ests = [linreg, ridge, lasso, rf, ada, clf]\n",
    "est_labels = np.array(['OLS','Ridge Tuned','Lasso Tuned', 'RF', 'AB (RF)', 'AB Tuned (RF)'])\n",
    "errvals = np.array([0.537, 0.534, 0.522, 0.57, 0.633, 0.651])\n",
    "\n",
    "#for e in ests:\n",
    "    #e.fit(X_train, Y_train)\n",
    "    #this_err = e.score(X_test, Y_test)\n",
    "    #errvals = np.append(errvals, this_err)\n",
    "    \n",
    "pos = np.arange(errvals.shape[0])\n",
    "srt = np.argsort(errvals)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(pos, errvals[srt], align = 'center', color='#E35A5C')\n",
    "plt.xticks(pos, est_labels[srt])\n",
    "plt.xlabel('Estimator')\n",
    "plt.ylabel('R^2 Value')\n",
    "plt.title('R^2 Model Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importance = rf_tuned.best_estimator_.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) - 1\n",
    "pvals = feature_importance[sorted_idx]\n",
    "pcols = X_train.columns[sorted_idx]\n",
    "plt.figure(figsize = (8,12))\n",
    "plt.barh(pos, pvals, align = 'center', color ='#E35A5C')\n",
    "plt.yticks(pos, pcols)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Predictor Importance Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calendar_dates = np.array(calendar['date'].unique())\n",
    "\n",
    "seasonality_data = np.empty([len(calendar_dates),1])\n",
    "median_seasonality_data = np.empty([len(calendar_dates),1])\n",
    "\n",
    "index = 0\n",
    "\n",
    "for item in calendar_dates:\n",
    "    calendar_dates[index] = datetime.strptime(item,'%m/%d/%Y').date()\n",
    "    seasonality_data[index, 0] = np.mean(calendar['price'].loc[calendar['date'] == item])\n",
    "    index += 1\n",
    "    \n",
    "for i in range(len(seasonality_data)):\n",
    "    median_seasonality_data[i] = np.median(seasonality_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(calendar_dates, seasonality_data, label='Data')\n",
    "ax.plot(calendar_dates, median_seasonality_data, label='Median', linestyle='-')\n",
    "ax.plot(calendar_dates, avg_week_price)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Nightly Price (USD)')\n",
    "ax.set_title('Average Nightly Price in 2015')\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_score = best.staged_score(X_test, Y_test)\n",
    "train_score = best.staged_score(X_train,Y_train)\n",
    "\n",
    "test_score_array = []\n",
    "train_score_array = []\n",
    "\n",
    "for i in test_score:\n",
    "    test_score_array.append(i)\n",
    "\n",
    "for i in train_score:\n",
    "    train_score_array.append(i)\n",
    "    \n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "plt.scatter(range(1,11), test_score_array, label = 'Testing Data', color ='#E35A5C')\n",
    "plt.scatter(range(1,11), train_score_array, label = 'Training Data', color ='#79CCCD')\n",
    "plt.title('Performance Over Boosting Iterations')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('R^2')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ran"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
